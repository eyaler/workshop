{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_2_mnist.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/workshop/blob/master/nn_2_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_2GbL_Pe7tP"
      },
      "source": [
        "# 2. How to train and test your network (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyIpe-gkeT3v"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.3\n",
        "!pip install livelossplot\n",
        "!pip install keras==2.2.5\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from keras import Input, Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.datasets import mnist, cifar10\n",
        "from keras.utils import to_categorical\n",
        "from livelossplot import PlotLossesKeras\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Acn_R2zeT36"
      },
      "source": [
        "# set random seeds for more reproducible results\n",
        "from numpy.random import seed\n",
        "seed(42)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(43)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxaa4gb5eT4I"
      },
      "source": [
        "# load dataset\n",
        "(raw_x_train, raw_y_train), (raw_x_test, raw_y_test) = mnist.load_data()\n",
        "print(raw_x_train.shape, raw_y_train.shape)\n",
        "print(raw_x_train.dtype, raw_y_train.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BHIGPQQeT4b"
      },
      "source": [
        "print(raw_x_train[0])\n",
        "print(raw_y_train[0])\n",
        "plt.imshow(raw_x_train[0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Ed-ak1eT4k"
      },
      "source": [
        "print(np.min(raw_x_train), np.max(raw_x_train), np.median(raw_x_train))\n",
        "print(np.unique(raw_y_train, return_counts=True))\n",
        "print(np.unique(raw_y_test, return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jucaKvFueT4u"
      },
      "source": [
        "n_classes = len(np.unique(raw_y_test))\n",
        "x_train, y_train = shuffle(raw_x_train, raw_y_train, random_state=44)\n",
        "x_train = x_train.reshape((60000, -1))/255\n",
        "x_test = raw_x_test.reshape((10000, -1))/255\n",
        "y_train = to_categorical(y_train, n_classes)\n",
        "y_test = to_categorical(raw_y_test, n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjYqdpq6CQ-u"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48U3hBFZVRyS"
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "#helper function to plot in grids\n",
        "def show_grid(image_list,nrows,ncols,label_list=None,show_labels=False,savename=None,figsize=(10,10),showaxis='off'):\n",
        "    if type(image_list) is not list:\n",
        "        if(image_list.shape[-1]==1):\n",
        "            image_list = [image_list[i,:,:,0] for i in range(image_list.shape[0])]\n",
        "        elif(image_list.shape[-1]==3):\n",
        "            image_list = [image_list[i,:,:,:] for i in range(image_list.shape[0])]\n",
        "    fig = plt.figure(None, figsize,frameon=False)\n",
        "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                     nrows_ncols=(nrows, ncols),  # creates 2x2 grid of axes\n",
        "                     axes_pad=0.3,  # pad between axes in inch.\n",
        "                     share_all=True,\n",
        "                     )\n",
        "    for i in range(nrows*ncols):\n",
        "        ax = grid[i]\n",
        "        ax.imshow(image_list[i],cmap='Greys_r')  # The AxesGrid object work as a list of axes.\n",
        "        ax.axis('off')\n",
        "        if show_labels:\n",
        "            ax.set_title(label_list[i])\n",
        "    if savename != None:\n",
        "        plt.savefig(savename,bbox_inches='tight')\n",
        "\n",
        "show_grid(raw_x_train[:32],4,8,label_list=np.squeeze(raw_y_train[:32]),show_labels=True,figsize=(20,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-G4idt2eT43"
      },
      "source": [
        "def get_model():\n",
        "    inputs = Input(shape=(784,))\n",
        "    f = Dense(32, activation='relu')(inputs)\n",
        "    f = Dense(32, activation='relu')(f)\n",
        "    outputs = Dense(n_classes, activation='softmax')(f)\n",
        "    return Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZkomlR9eT5B"
      },
      "source": [
        "model = get_model()\n",
        "print(model.summary())\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x=x_train, y=y_train, batch_size=128, epochs=20, validation_split=0.2, callbacks=[PlotLossesKeras()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F942G9BceT5U"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wn1zW9_eT5i"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "loss, acc = model.evaluate(x=x_test, y=y_test)\n",
        "print(loss, acc)\n",
        "targets = np.argmax(y_test, axis=-1)\n",
        "probabilities = model.predict(x=x_test)\n",
        "predictions = np.argmax(probabilities, axis=-1)\n",
        "cm = confusion_matrix(y_true=targets, y_pred=predictions)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_YB1tQOeT5w"
      },
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title = 'Normalized confusion matrix'\n",
        "    else:\n",
        "        title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    np.set_printoptions(precision=2)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "classes = np.arange(n_classes)\n",
        "plot_confusion_matrix(cm, classes=classes)\n",
        "plot_confusion_matrix(cm, classes=classes, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkCZz9zTeT57"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_true=targets, y_pred=predictions, labels=classes)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QFs2KjNeT6I"
      },
      "source": [
        "# top errors\n",
        "from sklearn.metrics import log_loss\n",
        "max_probs = np.max(probabilities, axis=-1)\n",
        "losses = [log_loss(y_true=y, y_pred=prob, eps=1e-7) for y,prob in zip(y_test,probabilities)]\n",
        "print('loss\\tindex\\ttrue\\tpredicted\\tprobability')\n",
        "top_errors = sorted(list(zip(losses, np.arange(10000), targets, predictions, max_probs)), reverse=True)[:32]\n",
        "titles = []\n",
        "inds = []\n",
        "for i,error in enumerate(top_errors):\n",
        "    titles.append('%d -> %d'%(error[2],error[3]))\n",
        "    inds.append(error[1])\n",
        "    if i<10:\n",
        "        print('%.04f\\t%d\\t%d\\t%d\\t\\t%.04f'%error)\n",
        "        #plt.figure()\n",
        "        #plt.imshow(raw_x_test[error[1]], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nvpcCstaqfV"
      },
      "source": [
        "show_grid(raw_x_test[inds[:32]],4,8,label_list=titles[:32],show_labels=True,figsize=(20,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAy0EpTw1_Fk"
      },
      "source": [
        "#EX:\n",
        "# 1. Look at the MNIST data\n",
        "# 2. overfit MNIST - get a very small training error\n",
        "# 3. add dropout regularization (rate=0.5) for each FC layer, and improve generalization\n",
        "# 4. evaluate on test and analyze top errors (largest per-sample logloss). what can be learned?\n",
        "# 5. check sensitivity to random seed\n",
        "# 6. fix code to work with CIFAR-10 converted to grey scale (look at the data, overfit, generalize, evaluate, etc). don't expect good results on CIFAR-10... try to get val_acc~0.5... try different network architetures"
      ]
    }
  ]
}
