{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/workshop/blob/master/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8umvDNC4pEbc"
      },
      "source": [
        "#BERT demo notebook\n",
        "###by Eyal Gruss\n",
        "###Hebrew support: Doron Adler\n",
        "### ⭐ New: Hebrew poetry glitcher - ShirBert! ⭐\n",
        "###Based on https://huggingface.co/transformers\n",
        "<img src='https://i.pinimg.com/originals/1a/38/8d/1a388d9b1e1ce42f424e60ce5b9d88ff.png' width=\"400px\"/>\n",
        "\n",
        "###Image credit: Doron Adler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OYMDBCMWoEF"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swp4_fNxl813"
      },
      "source": [
        "def run_model(text, embedding=False, use_cls=False):\n",
        "  # Tokenize input\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "  # Convert token to vocabulary indices\n",
        "  indexed_tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "  segments_ids = [0]*len(indexed_tokens)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "\n",
        "  if not embedding:\n",
        "    # Predict all tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = masked_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "\n",
        "    return indexed_tokens[1:-1], outputs[0][0][1:-1]\n",
        "  \n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "        encoded_layers, _ = bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    encoded_layers = encoded_layers[0].cpu()\n",
        "    if use_cls:\n",
        "      return encoded_layers[0] \n",
        "    return encoded_layers.mean(axis=0)\n",
        "\n",
        "def predict_missing_word(text, topn=10):\n",
        "  indexed_tokens, predictions = run_model(text)\n",
        "  \n",
        "  # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "  masked_index = indexed_tokens.index(tokenizer.convert_tokens_to_ids('[MASK]'))\n",
        "\n",
        "  predicted_inds = torch.argsort(-predictions[masked_index])\n",
        "  predicted_probs = [round(p.item(),4) for p in torch.softmax(predictions[masked_index], 0)[predicted_inds]]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
        "  return list(zip(predicted_tokens, predicted_probs))[:topn]\n",
        "\n",
        "def complete_missing_word(text):\n",
        "  word = predict_missing_word(text, topn=1)[0][0]\n",
        "  return text.replace('[MASK]', word)\n",
        "\n",
        "def get_word_probs(text):\n",
        "  indexed_tokens, predictions = run_model(text)\n",
        "  predicted_probs = [round(torch.softmax(predictions[i], 0)[j].item(),4) for i,j in enumerate(indexed_tokens)]\n",
        "  return list(zip(tokenizer.convert_ids_to_tokens(indexed_tokens), predicted_probs))\n",
        "\n",
        "def clean_heb(text):\n",
        "  vav = 'ו'\n",
        "  prefixes = 'מ|ש|ה|כ|ל|ב|כש'\n",
        "  return re.sub('\\\\b('+vav+'?(?:'+prefixes+')|'+vav+') ', '\\\\1', text)\n",
        "\n",
        "def fix_one_word(text, join_subwords=True, add_period=False, prevent_symbol=False, clean_hebrew=True):\n",
        "  added = False\n",
        "  if add_period and text[-1] not in '!?,.:;':\n",
        "    added = True\n",
        "    text += '.'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  if '[MASK]' in tokenized_text:\n",
        "    ind = tokenized_text.index('[MASK]')\n",
        "    bad_word = '[MASK]'\n",
        "  else:\n",
        "    probs = [p[1] for p in get_word_probs(text)]\n",
        "    if added:\n",
        "      probs[-1] = 1\n",
        "    ind = torch.argmin(torch.tensor(probs))\n",
        "    bad_word = tokenized_text[ind]\n",
        "    if join_subwords:\n",
        "      while ind>0 and tokenized_text[ind].startswith('##'):\n",
        "        ind -= 1\n",
        "      i = ind+1  \n",
        "      while i<len(tokenized_text) and tokenized_text[i].startswith('##'):\n",
        "        del tokenized_text[i]\n",
        "      if len(tokenized_text)!=len(probs):\n",
        "        bad_word = ''\n",
        "    tokenized_text[ind] = '[MASK]'\n",
        "    text = tokenizer.convert_tokens_to_string(tokenized_text)\n",
        "    text = tokenizer.clean_up_tokenization(text)\n",
        "    text = text.replace(' :', ':').replace(' ;', ';').replace(' )', ')').replace('( ', '(')\n",
        "  candidates = predict_missing_word(text, topn=None)\n",
        "  for word, _ in candidates:\n",
        "    if word != bad_word and word!=['UNK'] and (not prevent_symbol or re.search(r'\\w(?<!(\\d|_))',word) and (len(word)>1 or len(tokenized_text)>ind+1 and re.search(r'\\w(?<!(\\d|_))',tokenized_text[ind+1])) and (not word.startswith('##') or ind>0 and re.search(r'\\w(?<!(\\d|_))',tokenized_text[ind-1])) or len(bad_word)>1 and not re.search(r'\\w(?<!(\\d|_))',bad_word)):\n",
        "      break\n",
        "  text = text.replace('[MASK]', word).replace(' ##', '')\n",
        "  text = tokenizer.clean_up_tokenization(text)\n",
        "  text = text.replace(' :', ':').replace(' ;', ';').replace(' )', ')').replace('( ', '(')\n",
        "  if clean_hebrew:\n",
        "    text = clean_heb(text)\n",
        "  if added:\n",
        "    text = text[:-1]\n",
        "  return text\n",
        "\n",
        "def cosim(vec1, vec2):\n",
        "  return np.dot(vec1,vec2)/np.linalg.norm(vec1)/np.linalg.norm(vec2)\n",
        "\n",
        "def sent_sim(base_sent, compare_to, use_cls=False):\n",
        "  results = []\n",
        "  if type(compare_to)==str:\n",
        "    compare_to = [compare_to]\n",
        "  e1 = run_model(base_sent, embedding=True, use_cls=use_cls)\n",
        "  for s in compare_to:\n",
        "    e2 = run_model(s, embedding=True, use_cls=use_cls)\n",
        "    results.append(cosim(e1,e2))\n",
        "  if len(results)==1:\n",
        "    return results[0]\n",
        "  return results\n",
        "\n",
        "def mask_join(part1, part2, add_period=False):\n",
        "  s = part1 + ' [MASK] ' + part2  \n",
        "  if add_period and s[-1] not in '!?,.:;':\n",
        "    s += '.'\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyuTQ1VZbtaY",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import re\n",
        "\n",
        "model = 'bert-base-uncased' #@param ['bert-base-uncased', 'bert-large-uncased', 'bert-large-uncased-whole-word-masking', 'bert-base-multilingual-cased']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_j9L7hbxt2"
      },
      "source": [
        "predict_missing_word('The boy [MASK] to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQWt5nHpHuQ"
      },
      "source": [
        "predict_missing_word('Alex likes to have [MASK] with his best friend.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxz1yC4iuO5a"
      },
      "source": [
        "get_word_probs('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBnd7nKuQaB"
      },
      "source": [
        "fix_one_word('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdM53FBhNcgt"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd6EJq0qQRfx"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK].') #added period in the end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8a3AOMwQWst"
      },
      "source": [
        "complete_missing_word('The prime minister [MASK].')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBe6t-4sSjOl"
      },
      "source": [
        "get_word_probs('The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uudb8uwXTABM"
      },
      "source": [
        "get_word_probs('. The crime minister resigned.') #add period in beginning "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bakSezpnTSnl"
      },
      "source": [
        "fix_one_word('. The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sUcnoUm_J39"
      },
      "source": [
        "base_sent = 'she told me she loved me before she passed away'\n",
        "compare_to = [\n",
        "              'he told me he loved me before she passed away',\n",
        "              'he told me that you loved her before i passed away',\n",
        "              'i was very sad when my love died',\n",
        "              'you are my one and only love for eternity',\n",
        "              'i love pizza more than i love sex',\n",
        "              'we must have some pizza with onions',\n",
        "              'sieg heil',\n",
        "              'יאללה ביי'\n",
        "              ]   \n",
        "list(zip(sent_sim(base_sent, compare_to), compare_to))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guvdS_IO-t8n",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "model = 'TurkuNLP/wikibert-base-he-cased' #@param ['TurkuNLP/wikibert-base-he-cased', 'bert-base-multilingual-cased']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KyIkg466Pbq"
      },
      "source": [
        "s = 'ישראל [MASK] ולתפארת'\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOJ1W0GzRU_"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' #fixed order\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE43KB1sAQId"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' + '.' #added period\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf1xg6uV_4k-"
      },
      "source": [
        "#פרדי מרקורי מאסק זמר ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי'\n",
        "p2 = 'זמר ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Z8wRcdAUt8"
      },
      "source": [
        "#פרדי מרקורי היה מאסק ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי היה'\n",
        "p2 = 'ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh6KJn4s16bR"
      },
      "source": [
        "# שירברט\n",
        "\n",
        "def glitch_line(line, add_period=False, verbose=False):\n",
        "  hist = []\n",
        "  tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "  while line not in hist and len(tokens&set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)))>(len(tokens)>2):\n",
        "    hist.append(line)\n",
        "    line = fix_one_word(line, add_period=add_period, prevent_symbol=True, clean_hebrew=False)\n",
        "    if verbose:\n",
        "      print('>'*verbose+line)\n",
        "    if line in hist:\n",
        "      line = hist[-1]\n",
        "  return line\n",
        "\n",
        "def shirbert(text, add_period_to_short=False, verbose=False):\n",
        "  for line in text.strip().splitlines():\n",
        "    line = line.strip()\n",
        "    orig_line = line\n",
        "    tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "    line = glitch_line(line, add_period=len(tokens)>2 or add_period_to_short, verbose=1 if verbose else 0)\n",
        "    if len(tokens&set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)))>2 and line[-1] not in '!?,.:;':\n",
        "      line = glitch_line(orig_line, verbose=2 if verbose else 0)\n",
        "    if len(tokens&set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)))>2:\n",
        "      orig_line = orig_line.replace(sorted(tokens,key=len)[-1], '[MASK]', 1)\n",
        "      line = glitch_line(orig_line, add_period=True, verbose=3 if verbose else 0)\n",
        "    line = clean_heb(line)\n",
        "    print(line)  \n",
        "\n",
        "shirbert('''\n",
        "התקוה\n",
        "\n",
        "כל עוד בלבב פנימה\n",
        "נפש יהודי הומיה,\n",
        "ולפאתי מזרח קדימה\n",
        "עין לציון צופיה,\n",
        "עוד לא אבדה תקותנו,\n",
        "התקוה בת שנות אלפים,\n",
        "להיות עם חפשי בארצנו,\n",
        "ארץ ציון וירושלים.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNL24tWvhVOm"
      },
      "source": [
        "shirbert('''\n",
        "שתדעו.\n",
        "מאת אודיה רוזנק\n",
        "\n",
        "אני ועוד מיליון מובטלים\n",
        "רואים את התמונות שאתם מעלים לאינסטגרם,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים את עוגות הקצפת\n",
        "הלבנות שלכם, את המקררים העמוסים\n",
        "יוגורטים אפס אחוז שומן,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים אתכם מורחים חמאה\n",
        "על חלות תוצרת בית מקמח כוסמין\n",
        "ומוסיפים פרוסת סלמון,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים בלענו שעונים מעוררים\n",
        "טיק טק\n",
        "טיק טק\n",
        "טיק טק\n",
        "אתם שומעים?\n",
        "אני ועוד מיליון מובטלים מתכוננים לצאת\n",
        "לרחובות, לשבור לכם את החלונות\n",
        "לשרוף לכם את האסמים\n",
        "לתלוש לכם את הפנים, ולגלות את המסכות.\n",
        "שתדעו.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4qaw36Ag5a"
      },
      "source": [
        "shirbert('''\n",
        "האגם הגדול\n",
        "מאת רועי צ. ארד\n",
        " \n",
        "שוחה לבד בתוך האגם הגדול\n",
        "שוחה על בטני באגם הגדול\n",
        "שוחה על גבי באגם הגדול\n",
        "על צדי שוחה באגם הגדול\n",
        "מדוע איש לא מצטרף אלי באגם הגדול?\n",
        "אין גדר סביב האגם הגדול\n",
        "משתכשך בתוך אגם הגדול\n",
        "צולל בתוך אגם הגדול\n",
        "הדרך לדפוק את השיטה: האגם הגדול\n",
        "הצטרפי אלי אל האגם הגדול\n",
        "הצטרף אלי אל האגם הגדול\n",
        "מדוע אני לבד באגם הגדול?\n",
        "דבר לא מונע מכם לבוא אל האגם הגדול\n",
        "למשל אתה הקורא,\n",
        "אל נא תאמר \"אני רק הקורא\",\n",
        "הפשל המכנס, השלך החזיה,\n",
        "בוא עכשיו אל\n",
        "האגם הגדול!\n",
        "שחה עמוק בתוך האגם הגדול!\n",
        "שחה מהר בתוך האגם הגדול!\n",
        "שחה על גחונך בהאגם הגדול!\n",
        "שחה על העורף בהאגם הגדול!\n",
        "בוא עכשיו לכאן.\n",
        "פעם היו כאן רבים באגם הגדול\n",
        "אני היחיד שטבל באגם הגדול\n",
        "אפשר לטבוע בהאגם הגדול\n",
        "(אבל) אפשר למות מצחצוח יתר במרידול\n",
        "אז בואו בואו בואו אל האגם הגדול\n",
        "נצוף נצוף נצוף באגם הגדול\n",
        "אין כאן מים, רק קול\n",
        "נתחכּך בתוך האגם הגדול\n",
        "בשרכם יוטח בבשרי באגם הגדול\n",
        "בוא עכשיו לכאן.\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "כי אני לבד בתוך האגם הגדול\n",
        "כן, אני לבד באגם הגדול.\n",
        "אני לבד לבד לבד באגם הגדול\n",
        "לפעמים עם עוד כמה חברים\n",
        "מדוע אינכם מבינים שהכי סבבי באגם הגדול\n",
        "שהכי חינמי באגם הגדול\n",
        "שזה המקום היחיד בעיירה בלי גדרות, האגם הגדול\n",
        "ולא איזה אשד הפכפך, האגם הגדול\n",
        "והוא לא ממש גדול, האגם הגדול\n",
        "אפשר לשים אותו בבגאז' של פג'ו,\n",
        "בתא מטבעות מעור שסק\n",
        "בפנקסון סגול\n",
        "האגם הגדול האגם הגדול האגם הגדול\n",
        "הצטרפו אלי עכשיו לאגם הגדול\n",
        "הצטרפתו עמי באגם הגדול\n",
        "יש מקום לכולם באגם הגדול\n",
        "יש מקום לכולן באגם הגדול\n",
        "יש מקום לקולר באגם הגדול\n",
        "האגם הגד גד גד גדול\n",
        "האגם הגדול דול דול דול דול\n",
        "בואו אל האגם הגדול\n",
        "בואו אל האגם הגדול\n",
        "למה אתם נכנסים אל תוך האגם הגדול רק\n",
        "כשאני יוצא מהמים להתייבש?\n",
        "''', add_period_to_short=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}