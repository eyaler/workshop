{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/workshop/blob/master/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8umvDNC4pEbc"
      },
      "source": [
        "#BERT demo notebook\n",
        "###by Eyal Gruss\n",
        "###Hebrew support: Doron Adler\n",
        "###⭐ New: Hebrew poetry glitcher - ShirBert! ⭐\n",
        "###Based on https://huggingface.co/transformers\n",
        "<img src='https://i.pinimg.com/originals/1a/38/8d/1a388d9b1e1ce42f424e60ce5b9d88ff.png' width=\"400px\"/>\n",
        "\n",
        "###Image credit: Doron Adler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OYMDBCMWoEF"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swp4_fNxl813"
      },
      "source": [
        "def run_model(text, embedding=False, use_cls=False):\n",
        "  # Tokenize input\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "  # Convert token to vocabulary indices\n",
        "  indexed_tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "  segments_ids = [0]*len(indexed_tokens)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "\n",
        "  if not embedding:\n",
        "    # Predict all tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = masked_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "\n",
        "    return indexed_tokens[1:-1], outputs[0][0][1:-1]\n",
        "  \n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "        encoded_layers, _ = bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    encoded_layers = encoded_layers[0].cpu()\n",
        "    if use_cls:\n",
        "      return encoded_layers[0] \n",
        "    return encoded_layers.mean(axis=0)\n",
        "\n",
        "def predict_missing_word(text, topn=10):\n",
        "  indexed_tokens, predictions = run_model(text)\n",
        "  \n",
        "  # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "  masked_index = indexed_tokens.index(tokenizer.convert_tokens_to_ids('[MASK]'))\n",
        "\n",
        "  predicted_inds = torch.argsort(-predictions[masked_index])\n",
        "  predicted_probs = [round(p.item(),4) for p in torch.softmax(predictions[masked_index], 0)[predicted_inds]]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
        "  return list(zip(predicted_tokens, predicted_probs))[:topn]\n",
        "\n",
        "def complete_missing_word(text):\n",
        "  word = predict_missing_word(text, topn=1)[0][0]\n",
        "  return text.replace('[MASK]', word)\n",
        "\n",
        "def get_word_probs(text):\n",
        "  indexed_tokens, predictions = run_model(text)\n",
        "  predicted_probs = [round(torch.softmax(predictions[i], 0)[j].item(),4) for i,j in enumerate(indexed_tokens)]\n",
        "  return list(zip(tokenizer.convert_ids_to_tokens(indexed_tokens), predicted_probs))\n",
        "\n",
        "def fix_hebrew_prefix(text):\n",
        "  vav = 'ו'\n",
        "  prefixes = 'מ|ש|ה|כ|ל|ב|כש'\n",
        "  return re.sub('\\\\b('+vav+'?(?:'+prefixes+')|'+vav+') ', '\\\\1', text)\n",
        "\n",
        "def is_hebrew_prefix(text):\n",
        "  vav = 'ו'\n",
        "  prefixes = 'מ|ש|ה|כ|ל|ב|כש'\n",
        "  return text and re.fullmatch(vav+'?('+prefixes+')?', text)\n",
        "\n",
        "def fix_one_word(text, join_subwords=True, add_period=False, prevent_nonword=False, prefer_word=False, prevent_repeat=False, fix_hebrew=True):\n",
        "  added = False\n",
        "  if add_period and text[-1] not in '!?,.:;':\n",
        "    added = True\n",
        "    text += '.'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  if '[MASK]' in tokenized_text:\n",
        "    ind = tokenized_text.index('[MASK]')\n",
        "    bad_word = '[MASK]'\n",
        "  else:\n",
        "    probs = [p[1] for p in get_word_probs(text)]\n",
        "    if added:\n",
        "      probs[-1] = 1\n",
        "    ind = torch.argmin(torch.tensor(probs))\n",
        "    bad_word = tokenized_text[ind]\n",
        "    if join_subwords:\n",
        "      while ind>0 and tokenized_text[ind].startswith('##'):\n",
        "        ind -= 1\n",
        "      i = ind+1  \n",
        "      while i<len(tokenized_text) and tokenized_text[i].startswith('##'):\n",
        "        del tokenized_text[i]\n",
        "      if len(tokenized_text)!=len(probs):\n",
        "        bad_word = ''\n",
        "    tokenized_text[ind] = '[MASK]'\n",
        "    text = tokenizer.convert_tokens_to_string(tokenized_text)\n",
        "    text = tokenizer.clean_up_tokenization(text)\n",
        "    text = text.replace(' :', ':').replace(' ;', ';').replace(' )', ')').replace('( ', '(')\n",
        "  candidates = predict_missing_word(text, topn=None)\n",
        "  fix = bad_word\n",
        "  for word, _ in candidates:\n",
        "    if word != bad_word and (not prevent_nonword or word!='[UNK]' and (re.search(r'\\w(?<!(\\d|_))',word) or not prefer_word and bad_word and not re.search(r'\\w(?<!(\\d|_))',bad_word)) and (not is_hebrew_prefix(word) and (not re.fullmatch('[א-ת]',word) or not prefer_word and re.fullmatch('[א-ת]',bad_word)) or is_hebrew_prefix(word) and len(tokenized_text)>ind+1 and re.search(r'\\w(?<!(\\d|_))',tokenized_text[ind+1])) and (not word.startswith('##') or ind>0 and re.search(r'\\w(?<!(\\d|_))',tokenized_text[ind-1]))):\n",
        "      fix = word\n",
        "      if not prevent_repeat or word not in tokenized_text or len(word)==1:\n",
        "        break\n",
        "  text = text.replace('[MASK]', fix).replace(' ##', '')\n",
        "  text = tokenizer.clean_up_tokenization(text)\n",
        "  text = text.replace(' :', ':').replace(' ;', ';').replace(' )', ')').replace('( ', '(')\n",
        "  if fix_hebrew:\n",
        "    text = fix_hebrew_prefix(text)\n",
        "  if added:\n",
        "    text = text[:-1]\n",
        "  return text\n",
        "\n",
        "def cosim(vec1, vec2):\n",
        "  return np.dot(vec1,vec2)/np.linalg.norm(vec1)/np.linalg.norm(vec2)\n",
        "\n",
        "def sent_sim(base_sent, compare_to, use_cls=False):\n",
        "  results = []\n",
        "  if type(compare_to)==str:\n",
        "    compare_to = [compare_to]\n",
        "  e1 = run_model(base_sent, embedding=True, use_cls=use_cls)\n",
        "  for s in compare_to:\n",
        "    e2 = run_model(s, embedding=True, use_cls=use_cls)\n",
        "    results.append(cosim(e1,e2))\n",
        "  if len(results)==1:\n",
        "    return results[0]\n",
        "  return results\n",
        "\n",
        "def mask_join(part1, part2, add_period=False):\n",
        "  s = part1 + ' [MASK] ' + part2  \n",
        "  if add_period and s[-1] not in '!?,.:;':\n",
        "    s += '.'\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyuTQ1VZbtaY",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import re\n",
        "\n",
        "model = 'bert-base-uncased' #@param ['bert-base-uncased', 'bert-large-uncased', 'bert-large-uncased-whole-word-masking', 'bert-base-multilingual-cased']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_j9L7hbxt2"
      },
      "source": [
        "predict_missing_word('The boy [MASK] to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQWt5nHpHuQ"
      },
      "source": [
        "predict_missing_word('Alex likes to have [MASK] with his best friend.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxz1yC4iuO5a"
      },
      "source": [
        "get_word_probs('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBnd7nKuQaB"
      },
      "source": [
        "fix_one_word('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdM53FBhNcgt"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd6EJq0qQRfx"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK].') #added period in the end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8a3AOMwQWst"
      },
      "source": [
        "complete_missing_word('The prime minister [MASK].')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBe6t-4sSjOl"
      },
      "source": [
        "get_word_probs('The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uudb8uwXTABM"
      },
      "source": [
        "get_word_probs('. The crime minister resigned.') #add period in beginning "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bakSezpnTSnl"
      },
      "source": [
        "fix_one_word('. The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sUcnoUm_J39"
      },
      "source": [
        "base_sent = 'she told me she loved me before she passed away'\n",
        "compare_to = [\n",
        "              'he told me he loved me before she passed away',\n",
        "              'he told me that you loved her before i passed away',\n",
        "              'i was very sad when my love died',\n",
        "              'you are my one and only love for eternity',\n",
        "              'i love pizza more than i love sex',\n",
        "              'we must have some pizza with onions',\n",
        "              'sieg heil',\n",
        "              'יאללה ביי'\n",
        "              ]   \n",
        "list(zip(sent_sim(base_sent, compare_to), compare_to))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guvdS_IO-t8n",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "model = 'TurkuNLP/wikibert-base-he-cased' #@param ['TurkuNLP/wikibert-base-he-cased', 'bert-base-multilingual-cased']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KyIkg466Pbq"
      },
      "source": [
        "s = 'ישראל [MASK] ולתפארת'\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOJ1W0GzRU_"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' #fixed order\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE43KB1sAQId"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' + '.' #added period\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf1xg6uV_4k-"
      },
      "source": [
        "#פרדי מרקורי מאסק זמר ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי'\n",
        "p2 = 'זמר ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Z8wRcdAUt8"
      },
      "source": [
        "#פרדי מרקורי היה מאסק ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי היה'\n",
        "p2 = 'ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI5X7wNwJvdv"
      },
      "source": [
        "#שירברט - משבש (משברט) שירה עברית\n",
        "#Hebrew poetry glitcher - ShirBert\n",
        "###Best used with TurkuNLP/wikibert-base-he-cased model\n",
        "###Warning: poetic experimentation below this line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh6KJn4s16bR"
      },
      "source": [
        "def glitch_line(line, long_intersect=1, short_len=2, add_period=False, prevent_nonword=True, prefer_word=True, prevent_repeat=False, fix_hebrew=False, verbose=False):\n",
        "  hist = []\n",
        "  tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "  while line not in hist and len(tokens&set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)))>(long_intersect if len(tokens)>short_len else 0):\n",
        "    hist.append(line)\n",
        "    line = fix_one_word(line, add_period=add_period, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, fix_hebrew=fix_hebrew)\n",
        "    if verbose:\n",
        "      print('>'*verbose+line)\n",
        "    if line in hist:\n",
        "      line = hist[-1]\n",
        "  return line\n",
        "\n",
        "def shirbert(text, split_eos=False, add_period_to_short=False, prevent_nonword=True, prefer_word=True, prevent_repeat=False, verbose=False):\n",
        "  output = ''\n",
        "  if split_eos:\n",
        "    text = text.replace('. ','.\\n').replace('! ','!\\n').replace('? ','?\\n')\n",
        "  for line in text.strip().splitlines():\n",
        "    line = line.strip()\n",
        "    orig_line = line\n",
        "    tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "    line = glitch_line(line, add_period=len(tokens)>2 or add_period_to_short, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, verbose=1 if verbose else 0)\n",
        "    if len(tokens&set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)))>2 and line[-1] not in '!?,.:;':\n",
        "      line = glitch_line(orig_line, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, verbose=2 if verbose else 0)\n",
        "    if len(tokens&set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)))>2:\n",
        "      orig_line = orig_line.replace(sorted(tokens,key=len)[-1], '[MASK]', 1)\n",
        "      line = glitch_line(orig_line, add_period=True, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, verbose=3 if verbose else 0)\n",
        "    line = fix_hebrew_prefix(line)\n",
        "    print(line)\n",
        "    output += line+'\\n'\n",
        "  return output\n",
        "\n",
        "poem = shirbert('''\n",
        "התקוה\n",
        "\n",
        "כל עוד בלבב פנימה\n",
        "נפש יהודי הומיה,\n",
        "ולפאתי מזרח קדימה\n",
        "עין לציון צופיה,\n",
        "עוד לא אבדה תקותנו,\n",
        "התקוה בת שנות אלפים,\n",
        "להיות עם חפשי בארצנו,\n",
        "ארץ ציון וירושלים.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNL24tWvhVOm"
      },
      "source": [
        "poem = shirbert('''\n",
        "שתדעו.\n",
        "מאת אודיה רוזנק\n",
        "\n",
        "אני ועוד מיליון מובטלים\n",
        "רואים את התמונות שאתם מעלים לאינסטגרם,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים את עוגות הקצפת\n",
        "הלבנות שלכם, את המקררים העמוסים\n",
        "יוגורטים אפס אחוז שומן,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים אתכם מורחים חמאה\n",
        "על חלות תוצרת בית מקמח כוסמין\n",
        "ומוסיפים פרוסת סלמון,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים בלענו שעונים מעוררים\n",
        "טיק טק\n",
        "טיק טק\n",
        "טיק טק\n",
        "אתם שומעים?\n",
        "אני ועוד מיליון מובטלים מתכוננים לצאת\n",
        "לרחובות, לשבור לכם את החלונות\n",
        "לשרוף לכם את האסמים\n",
        "לתלוש לכם את הפנים, ולגלות את המסכות.\n",
        "שתדעו.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4qaw36Ag5a"
      },
      "source": [
        "poem = shirbert('''\n",
        "האגם הגדול.\n",
        "מאת רועי צ. ארד\n",
        " \n",
        "שוחה לבד בתוך האגם הגדול\n",
        "שוחה על בטני באגם הגדול\n",
        "שוחה על גבי באגם הגדול\n",
        "על צדי שוחה באגם הגדול\n",
        "מדוע איש לא מצטרף אלי באגם הגדול?\n",
        "אין גדר סביב האגם הגדול\n",
        "משתכשך בתוך אגם הגדול\n",
        "צולל בתוך אגם הגדול\n",
        "הדרך לדפוק את השיטה: האגם הגדול\n",
        "הצטרפי אלי אל האגם הגדול\n",
        "הצטרף אלי אל האגם הגדול\n",
        "מדוע אני לבד באגם הגדול?\n",
        "דבר לא מונע מכם לבוא אל האגם הגדול\n",
        "למשל אתה הקורא,\n",
        "אל נא תאמר \"אני רק הקורא\",\n",
        "הפשל המכנס, השלך החזיה,\n",
        "בוא עכשיו אל\n",
        "האגם הגדול!\n",
        "שחה עמוק בתוך האגם הגדול!\n",
        "שחה מהר בתוך האגם הגדול!\n",
        "שחה על גחונך בהאגם הגדול!\n",
        "שחה על העורף בהאגם הגדול!\n",
        "בוא עכשיו לכאן.\n",
        "פעם היו כאן רבים באגם הגדול\n",
        "אני היחיד שטבל באגם הגדול\n",
        "אפשר לטבוע בהאגם הגדול\n",
        "(אבל) אפשר למות מצחצוח יתר במרידול\n",
        "אז בואו בואו בואו אל האגם הגדול\n",
        "נצוף נצוף נצוף באגם הגדול\n",
        "אין כאן מים, רק קול\n",
        "נתחכּך בתוך האגם הגדול\n",
        "בשרכם יוטח בבשרי באגם הגדול\n",
        "בוא עכשיו לכאן.\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "כי אני לבד בתוך האגם הגדול\n",
        "כן, אני לבד באגם הגדול.\n",
        "אני לבד לבד לבד באגם הגדול\n",
        "לפעמים עם עוד כמה חברים\n",
        "מדוע אינכם מבינים שהכי סבבי באגם הגדול\n",
        "שהכי חינמי באגם הגדול\n",
        "שזה המקום היחיד בעיירה בלי גדרות, האגם הגדול\n",
        "ולא איזה אשד הפכפך, האגם הגדול\n",
        "והוא לא ממש גדול, האגם הגדול\n",
        "אפשר לשים אותו בבגאז' של פג'ו,\n",
        "בתא מטבעות מעור שסק\n",
        "בפנקסון סגול\n",
        "האגם הגדול האגם הגדול האגם הגדול\n",
        "הצטרפו אלי עכשיו לאגם הגדול\n",
        "הצטרפתו עמי באגם הגדול\n",
        "יש מקום לכולם באגם הגדול\n",
        "יש מקום לכולן באגם הגדול\n",
        "יש מקום לקולר באגם הגדול\n",
        "האגם הגד גד גד גדול\n",
        "האגם הגדול דול דול דול דול\n",
        "בואו אל האגם הגדול\n",
        "בואו אל האגם הגדול\n",
        "למה אתם נכנסים אל תוך האגם הגדול רק\n",
        "כשאני יוצא מהמים להתייבש?\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LASMqY88qer3"
      },
      "source": [
        "poem = shirbert('''\n",
        "הפדרציה הגלקטית...\n",
        "זו התחלת הקמת קונפדרציה בין גלקטית כשהמערכת הישנה מתפוררת לנגד עיניה של אוכלוסיית האדמה.\n",
        "הפדרציה הגלקטית מאשרת שכוחות האור משתלטים עכשיו.\n",
        "אנחנו עוברים למצב מוגבר של התעוררות ההמונים\n",
        "הלא מאמינים ואוכלוסיית כדור הארץ שבשליטה מוחית עוברים כעת לשלב הראשון של התעוררות.\n",
        "עבור רבים, זה יבוא בהלם והם יחוו באופן בלתי נמנע את הלילה האפל של הנשמה עכשיו.\n",
        "זה הכרחי כדי לעכל ולהוריד שכבות של תכנות ואמונות שווא.\n",
        "הבחירות בארה ′′ ב 2020 נוצלו כדי להפוך להפיכה הגדולה בהיסטוריה האוניברסלית.\n",
        "כל העולם צופה בפירוק המערכת הישנה. השחיתות עולה במהירות \n",
        "זה הקרב הסופי וניצחון האור מתקרב.\n",
        "האירועים הנוכחיים תוכננו בקפידה,  בין כוחות האור לברית הארץ.\n",
        "הסחף של הקליבר הזה חזק מספיק כדי לנער ולהעיר את הרדומים.\n",
        "אירועי 2020 אילצו רבים להעריך מחדש את מערכות האמונה שלהם.\n",
        "המצב הפוליטי בארצות הברית  אמריקה היא שיא האירועים שיביאו את נצחון האור על החושך.\n",
        "זה מסמן את מותה האולטימטיבי של מערכת מושחתת שאינה מכבדת את הארץ ולא מכבדת את החיים.\n",
        "עכשיו אתם עוברים לגיל הזהב של גאיה במהירות מואצת\n",
        "אל תתעסקו בדיונים פוליטיים עם אלה שעדיין נאחזים בתכנות המטריקס שלהם. זה יגרור אותכם למטה ויוריד את התדר שלכם.\n",
        "בשלב זה, השולחנות מסתובבים במהירות ואפילו ה-MSM בקרוב יאלצו לדווח על המשוחדים.\n",
        "כוחות האור בשליטה מלאה ויתחילו להתאים את הנרטיב עכשיו.\n",
        "מתבקש ממכם לסייע בשמירה על התדר שלכם ולהיאחז בחזון החירות, השלום והאהבה שלכם למין האנושי.\n",
        "התגלות האמת קרובה. כל החושך יוצא לאור. רבים יהיו הרוסים וירגישו אבודים או לכודים.\n",
        "התכנות חמור ואנשים על פני האדמה מזדהים עם אמונותיהם. כשהאמונות האלה מתחילות להימחץ הם יתבלבלו בלי לדעת מי הם.\n",
        "כאן אנחנו צריכים שתחזיקו את האור. אתם אלה שבחרתם במשימה הזאת. זו הסיבה שאתה ער ונאור כדי שתוכל להיות הדרך-, נושאי האור, והדרך הגלקטית שחבריך תושבי האדמה זקוקים כעת ליותר מתמיד.\n",
        "רבים יתחילו להגיע אליכם, מבקשים תמיכה מוסרית וחמלה בזמנים מלחיצים אלו.\n",
        "רוצים להבטיח לכם שאנחנו רואים ומעריכים את כל העבודה הקשה שעשיתם כדי לעזור להעלות את רטט הארץ.\n",
        "התודעה הקולקטיבית מוכנה לקבל את קוד האור הגבוה יותר עכשיו.\n",
        "ממשיכים לפוצץ את כדור הארץ עם  אור של פוטון גאמה. קודים אלה נושאים תדר שנועד להפוך את תהליך המעבר הזה לחלק ככל האפשר - או ככל שהתודעה האישית מאפשרת.\n",
        "למי שכבר עבר תהליך ההתעוררות, הקודים האלה יפעילו את מי שאתם ויניחו אתכם על כביש העל לעליית תלת מימד.\n",
        "הפירוק הרשמי של הפרדיגמה הישנה התחיל כשמנהיגי הקאבל נפלו למלכודת של עצמם.\n",
        "לשמוח יקרים.\n",
        "אתם תשכחו את כל הקשיים שהייתם צריכים לסבול ולחוות הרמוניה ואהבה, בניגוד לשום דבר שאי פעם חלמתם עליו.\n",
        "שפע עולמך יוחזר אליך וינוח לרגליך. זה התחיל.\n",
        "אנחנו אוהבים אותך. אנחנו כאן איתכם.\n",
        "אנחנו בשליטה עכשיו.\n",
        "זהו נצחון האור על החושך ועל בואו של שחר גאיה החדש.\n",
        "א ' הו\n",
        "אורורה ריי\n",
        "שגרירת הפדרציה הגלקטית\n",
        "''', split_eos=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}