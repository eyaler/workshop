{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/workshop/blob/master/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8umvDNC4pEbc"
      },
      "source": [
        "#BERT demo notebook\n",
        "###by Eyal Gruss\n",
        "###Hebrew support: Doron Adler\n",
        "###⭐ New: Hebrew poetry glitcher - ShirBert! ⭐\n",
        "###Based on https://huggingface.co/transformers\n",
        "<img src='https://i.pinimg.com/originals/1a/38/8d/1a388d9b1e1ce42f424e60ce5b9d88ff.png' width=\"400px\"/>\n",
        "\n",
        "###Image credit: Doron Adler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OYMDBCMWoEF"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swp4_fNxl813"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import re\n",
        "\n",
        "def run_model(text, add_special_tokens=True, embedding=False, use_cls=False):\n",
        "  # Tokenize input\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "  # Convert token to vocabulary indices\n",
        "  indexed_tokens = tokenizer.encode(text, add_special_tokens=add_special_tokens or use_cls)\n",
        "\n",
        "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "  segments_ids = [0]*len(indexed_tokens)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "\n",
        "  if not embedding:\n",
        "    # Predict all tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = masked_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    if add_special_tokens:\n",
        "      return indexed_tokens[1:-1], outputs[0][0][1:-1]  \n",
        "    return indexed_tokens, outputs[0][0]\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "  encoded_layers = outputs.last_hidden_state[0].cpu()\n",
        "  if use_cls:\n",
        "    return encoded_layers[0] \n",
        "  return encoded_layers.mean(axis=0)\n",
        "\n",
        "def predict_missing_word(text, topn=10, add_special_tokens=True):\n",
        "  indexed_tokens, predictions = run_model(text, add_special_tokens=add_special_tokens)\n",
        "  \n",
        "  # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "  masked_index = indexed_tokens.index(tokenizer.convert_tokens_to_ids('[MASK]'))\n",
        "\n",
        "  predicted_inds = torch.argsort(-predictions[masked_index])\n",
        "  predicted_probs = [round(p.item(), 4) for p in torch.softmax(predictions[masked_index], 0)[predicted_inds]]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
        "  return list(zip(predicted_tokens, predicted_probs))[:topn]\n",
        "\n",
        "def complete_missing_word(text, add_special_tokens=True):\n",
        "  word = predict_missing_word(text, topn=1, add_special_tokens=add_special_tokens)[0][0]\n",
        "  return text.replace('[MASK]', word)\n",
        "\n",
        "def join_clean(tokens):\n",
        "  text = ' '.join(tokens)\n",
        "  text = text.replace('ך ##', 'כ').replace('ם ##', 'מ').replace('נ ##', 'ן').replace('ף ##', 'פ').replace('ץ ##', 'צ').replace(' ##', '')\n",
        "  text = text.replace(' ##', '').strip()\n",
        "  text = tokenizer.clean_up_tokenization(text)\n",
        "  text = text.replace(' :', ':').replace(' ;', ';').replace(' )', ')').replace('( ', '(')\n",
        "  return text\n",
        "\n",
        "def get_word_probs(text, add_special_tokens=True, with_masking=False):\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  if with_masking:\n",
        "    indexed_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    predicted_probs = []\n",
        "    for i in range(len(tokenized_text)):\n",
        "      masked_tokens = tokenized_text[:]\n",
        "      masked_tokens[i] = '[MASK]'\n",
        "      text = join_clean(masked_tokens)\n",
        "      _, predictions = run_model(text, add_special_tokens=add_special_tokens)\n",
        "      predicted_probs.append(round(torch.softmax(predictions[i], 0)[indexed_tokens[i]].item(), 4))\n",
        "  else:\n",
        "    indexed_tokens, predictions = run_model(text, add_special_tokens=add_special_tokens)\n",
        "    predicted_probs = [round(torch.softmax(predictions[i], 0)[j].item(), 4) for i,j in enumerate(indexed_tokens)]\n",
        "  return list(zip(tokenized_text, predicted_probs))\n",
        "\n",
        "def fix_hebrew_prefix(text):\n",
        "  vav = 'ו'\n",
        "  prefixes = 'מ|ש|ה|כ|ל|ב|כש'\n",
        "  return re.sub('\\\\b('+vav+'?(?:'+prefixes+')|'+vav+') ', '\\\\1', text)\n",
        "\n",
        "def is_hebrew_prefix(text):\n",
        "  vav = 'ו'\n",
        "  prefixes = 'מ|ש|ה|כ|ל|ב|כש'\n",
        "  return text and re.fullmatch(vav+'?('+prefixes+')?', text)\n",
        "\n",
        "def fix_one_word(text, add_special_tokens=True, with_masking=False, join_subwords=True, add_period=False, prevent_nonword=False, prefer_word=False, prevent_repeat=False, fix_hebrew=True):\n",
        "  added = False\n",
        "  if add_period and text[-1] not in '!?,.:;':\n",
        "    added = True\n",
        "    text += '.'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  if '[MASK]' in tokenized_text:\n",
        "    ind = tokenized_text.index('[MASK]')\n",
        "    bad_word = '[MASK]'\n",
        "  else:\n",
        "    probs = [p[1] for p in get_word_probs(text, add_special_tokens=add_special_tokens, with_masking=with_masking)]\n",
        "    if added:\n",
        "      probs[-1] = 1\n",
        "    ind = torch.argmin(torch.tensor(probs))\n",
        "    if join_subwords:\n",
        "      while ind>0 and tokenized_text[ind].startswith('##'):\n",
        "        ind -= 1\n",
        "      bad_word = tokenized_text[ind]\n",
        "      i = ind+1  \n",
        "      while i<len(tokenized_text) and tokenized_text[i].startswith('##'):\n",
        "        bad_word += ' '+tokenized_text[ind]\n",
        "        del tokenized_text[i]\n",
        "    else:\n",
        "      bad_word = tokenized_text[ind]\n",
        "    tokenized_text[ind] = '[MASK]'\n",
        "    text = join_clean(tokenized_text)\n",
        "  candidates = predict_missing_word(text, topn=None, add_special_tokens=add_special_tokens)\n",
        "  fix = bad_word\n",
        "  for word, _ in candidates:\n",
        "    if word != bad_word and (not prevent_nonword or word!='[UNK]' and (re.search(r'\\w(?<!(\\d|_))',word) and (tokenized_text[ind][-1] not in 'ךםןףץ' or len(tokenized_text)==ind+1 or tokenized_text[ind+1].startswith('##')) or not prefer_word and bad_word and not re.search(r'\\w(?<!(\\d|_))',bad_word)) and (not is_hebrew_prefix(word) and (not re.fullmatch('[א-ת]',word) or not prefer_word and re.fullmatch('[א-ת]',bad_word)) or is_hebrew_prefix(word) and len(tokenized_text)>ind+1 and re.search(r'\\w(?<!(\\d|_))',tokenized_text[ind+1])) and (not word.startswith('##') or ind>0 and re.search(r'\\w(?<!(\\d|_))',tokenized_text[ind-1]))):\n",
        "      fix = word\n",
        "      if not prevent_repeat or word not in tokenized_text or len(word)==1:\n",
        "        break\n",
        "  text = text.replace('[MASK]', fix)\n",
        "  text = join_clean([text])\n",
        "  if fix_hebrew:\n",
        "    text = fix_hebrew_prefix(text)\n",
        "  if added:\n",
        "    text = text[:-1]\n",
        "  return text\n",
        "\n",
        "def cosim(vec1, vec2):\n",
        "  return np.dot(vec1,vec2)/np.linalg.norm(vec1)/np.linalg.norm(vec2)\n",
        "\n",
        "def sent_sim(base_sent, compare_to, add_special_tokens=True, use_cls=False):\n",
        "  results = []\n",
        "  if type(compare_to)==str:\n",
        "    compare_to = [compare_to]\n",
        "  e1 = run_model(base_sent, add_special_tokens=add_special_tokens, embedding=True, use_cls=use_cls)\n",
        "  for s in compare_to:\n",
        "    e2 = run_model(s, add_special_tokens=add_special_tokens, embedding=True, use_cls=use_cls)\n",
        "    results.append(cosim(e1,e2))\n",
        "  if len(results)==1:\n",
        "    return results[0]\n",
        "  return results\n",
        "\n",
        "def mask_join(part1, part2, add_period=False):\n",
        "  s = part1 + ' [MASK] ' + part2  \n",
        "  if add_period and s[-1] not in '!?,.:;':\n",
        "    s += '.'\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyuTQ1VZbtaY",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "model = 'bert-base-uncased' #@param ['bert-base-uncased', 'bert-large-uncased', 'bert-large-uncased-whole-word-masking', 'bert-base-multilingual-cased']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_j9L7hbxt2"
      },
      "source": [
        "predict_missing_word('The boy [MASK] to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQWt5nHpHuQ"
      },
      "source": [
        "predict_missing_word('Alex likes to have [MASK] with his best friend.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxz1yC4iuO5a"
      },
      "source": [
        "get_word_probs('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nVlZtLIbLw-"
      },
      "source": [
        "get_word_probs('The boy want to his school.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBnd7nKuQaB"
      },
      "source": [
        "fix_one_word('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO1wCIIE9NjH"
      },
      "source": [
        "get_word_probs('The boy want to his school')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqi-IBCgc8zO"
      },
      "source": [
        "get_word_probs('The boy want to his school', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfYBa7LJOgI7"
      },
      "source": [
        "get_word_probs('The boy returned to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhi5wBgNdHit"
      },
      "source": [
        "get_word_probs('The boy returned to his school.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqaKEf9WJwoi"
      },
      "source": [
        "get_word_probs('he said')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_8m7skhdavd"
      },
      "source": [
        "get_word_probs('he said', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpm0trhj9O6x"
      },
      "source": [
        "get_word_probs('he said.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gXeubd_dk-x"
      },
      "source": [
        "get_word_probs('he said.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdM53FBhNcgt"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd6EJq0qQRfx"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK].') #added period in the end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8a3AOMwQWst"
      },
      "source": [
        "complete_missing_word('The prime minister [MASK].')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBe6t-4sSjOl"
      },
      "source": [
        "get_word_probs('The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm6PeFkNdw4Z"
      },
      "source": [
        "get_word_probs('The crime minister resigned.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uudb8uwXTABM"
      },
      "source": [
        "get_word_probs('. The crime minister resigned.') #add period in beginning "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7BcVeMBd4_c"
      },
      "source": [
        "get_word_probs('. The crime minister resigned.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bakSezpnTSnl"
      },
      "source": [
        "fix_one_word('. The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sUcnoUm_J39"
      },
      "source": [
        "base_sent = 'she told me she loved me before she passed away'\n",
        "compare_to = [\n",
        "              'he told me he loved me before she passed away',\n",
        "              'he told me that you loved her before i passed away',\n",
        "              'i was very sad when my love died',\n",
        "              'you are my one and only love for eternity',\n",
        "              'i love pizza more than i love sex',\n",
        "              'we must have some pizza with onions',\n",
        "              'sieg heil',\n",
        "              'יאללה ביי'\n",
        "              ]   \n",
        "sorted(zip(sent_sim(base_sent, compare_to), compare_to),reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guvdS_IO-t8n",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "model = 'TurkuNLP/wikibert-base-he-cased' #@param ['TurkuNLP/wikibert-base-he-cased', 'bert-base-multilingual-cased']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KyIkg466Pbq"
      },
      "source": [
        "s = 'ישראל [MASK] ולתפארת'\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOJ1W0GzRU_"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' #fixed order\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE43KB1sAQId"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' + '.' #added period\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkcmqNYj9X-e"
      },
      "source": [
        "get_word_probs('לא רוצה')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyg_S_LqeNqe"
      },
      "source": [
        "get_word_probs('לא רוצה', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duyf_c2M9aeG"
      },
      "source": [
        "get_word_probs('לא רוצה.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NlkoE_JeSnY"
      },
      "source": [
        "get_word_probs('לא רוצה.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf1xg6uV_4k-"
      },
      "source": [
        "#פרדי מרקורי מאסק זמר ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי'\n",
        "p2 = 'זמר ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Z8wRcdAUt8"
      },
      "source": [
        "#פרדי מרקורי היה מאסק ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי היה'\n",
        "p2 = 'ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI5X7wNwJvdv"
      },
      "source": [
        "#שירברט - משבש (משברט) שירה עברית\n",
        "#Hebrew poetry glitcher - ShirBert\n",
        "###Best used with TurkuNLP/wikibert-base-he-cased model\n",
        "###Warning: poetic experimentation below this line\n",
        "<img src='https://i.pinimg.com/originals/e2/8d/08/e28d0819c670b164eceb38d7e5acb466.jpg' width=\"400px\"/>\n",
        "\n",
        "###Image credit: [thismuppetdoesnotexist.com](https://thismuppetdoesnotexist.com) by Doron Adler and EG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh6KJn4s16bR"
      },
      "source": [
        "def glitch_line(line, long_intersect=1, short_count=2, with_masking=False, add_period=False, prevent_nonword=True, prefer_word=True, prevent_repeat=False, fix_hebrew=False, verbose=False):\n",
        "  hist = []\n",
        "  tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "  while line not in hist and len(tokens&set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)))>(long_intersect if len(tokens)>short_count else 0):\n",
        "    hist.append(line)\n",
        "    line = fix_one_word(line, with_masking=with_masking, add_period=add_period, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, fix_hebrew=fix_hebrew)\n",
        "    if verbose:\n",
        "      print('>'*verbose+line)\n",
        "    if line in hist:\n",
        "      line = hist[-1]\n",
        "  return line\n",
        "\n",
        "def shirbert(text, remove_nikud=True, split_eos=False, with_masking=False, add_period_to_short=False, prevent_nonword=True, prefer_word=True, prevent_repeat=False, verbose=False):\n",
        "  output = ''\n",
        "  min_sim = 1\n",
        "  levels = set()\n",
        "  if remove_nikud:\n",
        "    text = re.sub('[\\u0591-\\u05bd\\u05bf-\\u05c2\\u05c4\\u05c5\\u05c7]','',text) # remove all nikud and teamim except makaf, sof-pasuk, nun-hafukha\n",
        "  if split_eos:\n",
        "    text = text.replace('. ','.\\n').replace('! ','!\\n').replace('? ','?\\n')\n",
        "  for line in text.strip().splitlines():\n",
        "    line = line.strip()\n",
        "    orig_line = line\n",
        "    short_tokens = re.findall(r\"\\b(?:\\w'?){2,}\\b\", line)\n",
        "    num_tokens = len(short_tokens)\n",
        "    short_tokens = set(short_tokens)\n",
        "    long_tokens = {t for t in short_tokens if len(t)>=3}\n",
        "    add_period = num_tokens>2 or add_period_to_short and line[-1] not in '!?,.:;'\n",
        "    line = glitch_line(line, with_masking=with_masking, add_period=add_period, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, verbose=1 if verbose else 0)\n",
        "    sim = sent_sim(orig_line, line)\n",
        "    min_sim = min(min_sim,sim)\n",
        "    levels.add(1)\n",
        "    if verbose:\n",
        "      print(f'>sim={sim:.2f}')\n",
        "    tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "    if (len(long_tokens&tokens)>2 or len(short_tokens&tokens)>0 and len(short_tokens)<=2) and add_period:\n",
        "      line = glitch_line(orig_line, with_masking=with_masking, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, verbose=2 if verbose else 0)\n",
        "      sim = sent_sim(orig_line, line)\n",
        "      min_sim = min(min_sim,sim)\n",
        "      levels.add(2)\n",
        "      if verbose:\n",
        "        print(f'>>sim={sim:.2f}')\n",
        "    tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "    if len(long_tokens&tokens)>2 or len(short_tokens&tokens)>0 and len(short_tokens)<=2:\n",
        "      mask_line = orig_line.replace(sorted(short_tokens,key=len)[-1], '[MASK]', 1)\n",
        "      add_period = num_tokens>2 or add_period_to_short and mask_line[-1] not in '!?,.:;'\n",
        "      line = glitch_line(mask_line, with_masking=with_masking, add_period=add_period, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, verbose=3 if verbose else 0)\n",
        "      sim = sent_sim(orig_line, line)\n",
        "      min_sim = min(min_sim,sim)\n",
        "      levels.add(3)\n",
        "      if verbose:\n",
        "        print(f'>>>sim={sim:.2f}')\n",
        "      tokens = set(re.findall(r\"\\b(?:\\w'?){2,}\\b\", line))\n",
        "      if len(short_tokens&tokens)>0 and len(short_tokens)<=2 and add_period:\n",
        "        line = glitch_line(mask_line, with_masking=with_masking, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, verbose=4 if verbose else 0)\n",
        "        sim = sent_sim(orig_line, line)\n",
        "        min_sim = min(min_sim,sim)\n",
        "        levels.add(4)\n",
        "        if verbose:\n",
        "          print(f'>>>>sim={sim:.2f}')\n",
        "    line = fix_hebrew_prefix(line)\n",
        "    print(line)\n",
        "    output += line+'\\n'\n",
        "  if verbose:\n",
        "    print(f'>>>min_sim={min_sim:.2f} levels={sorted(levels)}')\n",
        "  return output\n",
        "\n",
        "poem = shirbert('''\n",
        "התקוה\n",
        "נפתלי ה. אימבר\n",
        "\n",
        "כל עוד בלבב פנימה\n",
        "נפש יהודי הומיה,\n",
        "ולפאתי מזרח קדימה\n",
        "עין לציון צופיה,\n",
        "עוד לא אבדה תקותנו,\n",
        "התקוה בת שנות אלפים,\n",
        "להיות עם חפשי בארצנו,\n",
        "ארץ ציון וירושלים.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWR8GX5zw57o"
      },
      "source": [
        "poem = shirbert('''\n",
        "_מעילי הפשוט ופנס על הגשר\n",
        "ליל הסתיו ושפתי הלחות מני גשם\n",
        "כך ראית אותי ראשונה התזכור\n",
        "והיה לי ברור כמו שתיים ושתיים\n",
        "כי אהיה בשבילך כמו לחם ומים\n",
        "וכאל מים ולחם אלי תחזו\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oFL15KoO0rU"
      },
      "source": [
        "poem = shirbert('''\n",
        "מהפכה, מתהפכים כמו 69\n",
        "היום כן משתלם הפשע\n",
        "אני מסתובב ברחובות מחפש מהות\n",
        "אבל הכל חוזר חביבי כמו תקליט שרוט\n",
        "ואינפלציה כבר מיפלציה מכאיבה כמו אפילציה\n",
        "מה זה אינטגרציה?! השיטה מניפולציה\n",
        "ואתה מחכה שמשהו יפסיק ת'כאב\n",
        "כמו כדור 9 מילימטר בתוך הלב\n",
        "אנחנו מטרות נעות\n",
        "אין שטרות - אין בעיות\n",
        "שטויות, הבטחות במים נכתבות\n",
        "חולמים בעיניים פתוחות - לרווחה\n",
        "פאק משרד הרווחה - לחם, עבודה\n",
        "תמונות, צרחות, סירנות ויריות\n",
        "אפילו בשכונת התקווה אבדו כל התקוות\n",
        "יורד על הברכיים ''אלי אנא''\n",
        "הנה זה בא (מי בא?) זינזאנה!\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNL24tWvhVOm"
      },
      "source": [
        "poem = shirbert('''\n",
        "שתדעו.\n",
        "מאת אודיה רוזנק\n",
        "\n",
        "אני ועוד מיליון מובטלים\n",
        "רואים את התמונות שאתם מעלים לאינסטגרם,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים את עוגות הקצפת\n",
        "הלבנות שלכם, את המקררים העמוסים\n",
        "יוגורטים אפס אחוז שומן,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים אתכם מורחים חמאה\n",
        "על חלות תוצרת בית מקמח כוסמין\n",
        "ומוסיפים פרוסת סלמון,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים בלענו שעונים מעוררים\n",
        "טיק טק\n",
        "טיק טק\n",
        "טיק טק\n",
        "אתם שומעים?\n",
        "אני ועוד מיליון מובטלים מתכוננים לצאת\n",
        "לרחובות, לשבור לכם את החלונות\n",
        "לשרוף לכם את האסמים\n",
        "לתלוש לכם את הפנים, ולגלות את המסכות.\n",
        "שתדעו.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4qaw36Ag5a"
      },
      "source": [
        "poem = shirbert('''\n",
        "האגם הגדול.\n",
        "מאת רועי צ. ארד\n",
        " \n",
        "שוחה לבד בתוך האגם הגדול\n",
        "שוחה על בטני באגם הגדול\n",
        "שוחה על גבי באגם הגדול\n",
        "על צדי שוחה באגם הגדול\n",
        "מדוע איש לא מצטרף אלי באגם הגדול?\n",
        "אין גדר סביב האגם הגדול\n",
        "משתכשך בתוך אגם הגדול\n",
        "צולל בתוך אגם הגדול\n",
        "הדרך לדפוק את השיטה: האגם הגדול\n",
        "הצטרפי אלי אל האגם הגדול\n",
        "הצטרף אלי אל האגם הגדול\n",
        "מדוע אני לבד באגם הגדול?\n",
        "דבר לא מונע מכם לבוא אל האגם הגדול\n",
        "למשל אתה הקורא,\n",
        "אל נא תאמר \"אני רק הקורא\",\n",
        "הפשל המכנס, השלך החזיה,\n",
        "בוא עכשיו אל\n",
        "האגם הגדול!\n",
        "שחה עמוק בתוך האגם הגדול!\n",
        "שחה מהר בתוך האגם הגדול!\n",
        "שחה על גחונך בהאגם הגדול!\n",
        "שחה על העורף בהאגם הגדול!\n",
        "בוא עכשיו לכאן.\n",
        "פעם היו כאן רבים באגם הגדול\n",
        "אני היחיד שטבל באגם הגדול\n",
        "אפשר לטבוע בהאגם הגדול\n",
        "(אבל) אפשר למות מצחצוח יתר במרידול\n",
        "אז בואו בואו בואו אל האגם הגדול\n",
        "נצוף נצוף נצוף באגם הגדול\n",
        "אין כאן מים, רק קול\n",
        "נתחכך בתוך האגם הגדול\n",
        "בשרכם יוטח בבשרי באגם הגדול\n",
        "בוא עכשיו לכאן.\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "כי אני לבד בתוך האגם הגדול\n",
        "כן, אני לבד באגם הגדול.\n",
        "אני לבד לבד לבד באגם הגדול\n",
        "לפעמים עם עוד כמה חברים\n",
        "מדוע אינכם מבינים שהכי סבבי באגם הגדול\n",
        "שהכי חינמי באגם הגדול\n",
        "שזה המקום היחיד בעיירה בלי גדרות, האגם הגדול\n",
        "ולא איזה אשד הפכפך, האגם הגדול\n",
        "והוא לא ממש גדול, האגם הגדול\n",
        "אפשר לשים אותו בבגאז' של פג'ו,\n",
        "בתא מטבעות מעור שסק\n",
        "בפנקסון סגול\n",
        "האגם הגדול האגם הגדול האגם הגדול\n",
        "הצטרפו אלי עכשיו לאגם הגדול\n",
        "הצטרפתו עמי באגם הגדול\n",
        "יש מקום לכולם באגם הגדול\n",
        "יש מקום לכולן באגם הגדול\n",
        "יש מקום לקולר באגם הגדול\n",
        "האגם הגד גד גד גדול\n",
        "האגם הגדול דול דול דול דול\n",
        "בואו אל האגם הגדול\n",
        "בואו אל האגם הגדול\n",
        "למה אתם נכנסים אל תוך האגם הגדול רק\n",
        "כשאני יוצא מהמים להתייבש?\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvXNbOxV9K4t"
      },
      "source": [
        "poem = shirbert('''\n",
        "שני ארזי\n",
        "\n",
        "מיקרוס פר בנרה הסינדוק משלם\n",
        "קולו דנצירסמי אוטונומאנטילן\n",
        "שוקולרצח בעד דם לא לוחשת מיד!\n",
        "הציבני לגוף אהיה אל אחד!\n",
        "הצילני מדם וזוועות משיחין\n",
        "\n",
        "סעודת רשעים בגבולות היומרין\n",
        "לא תראה בי אח מוות משתנה וזהה\n",
        "מה עסוק בי רום דע את גבולותיך בשן!\n",
        "אל תנשך אל תרצח אל תגנוב מתחתי\n",
        "לא תלויה בכוחו עבדך אשריי \n",
        "\n",
        "לא נתלה בעור מוות ודום עומק גופך\n",
        "מה שלי ונגזר עת דפך באפך!\n",
        "לא נשבר ורועד השימני בצל\n",
        "אשתנה אתגלד בתנועות יד ואל\n",
        "ובשמי אתרשם ואראה חיסורים\n",
        "\n",
        "לא תרביץ כשתשמע חיבורים אסורים!\n",
        "מה נגזר בעול מוות משתנה ונכנס\n",
        "היונה התפשטה במוחו הדרדס\n",
        "ואנס ותם רוח מתעקש לעוד דם\n",
        "לא שלום אדוני לא ירצח אף פעם\n",
        "\n",
        "לא ארשה שתרצח אדוני את דמי\n",
        "מה יקרה אם תיקח נשרים מבשרי\n",
        "והרצח ינגע באורות שארית\n",
        "אנדרגראונד נס ובא מתקשר לאורית\n",
        "מי זה כאן הפולש בשיחת הבירור\n",
        "\n",
        "מה אציל מתבייש רוטב אש הסיפור\n",
        "לא מקריח יודע בגד דמות שם צמוד\n",
        "שערות ופשרות רקדנית על עמוד\n",
        "לא נכנסת פואנטה בהרגל אפשרי\n",
        "לא יוצא אל ומטא חידוש השני\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PZ2bOD5HBF"
      },
      "source": [
        "poem = shirbert('''\n",
        "גן נעול\n",
        "\n",
        "מִי אַתָּה? מַדּוּעַ יָד מוּשֶׁטֶת\n",
        "לֹא פּוֹגֶשֶׁת יַד אָחוֹת?\n",
        "וְעֵינַיִם אַךְ תַּמְתֵּנָּה רֶגַע\n",
        "וְהִנֵה שָׁפְלוּ כְּבָר נְבוֹכוֹת.\n",
        "\n",
        "גַּן נָעוּל. לֹא שְׁבִיל אֵלָיו, לֹא דֶרֶךְ.\n",
        "גַּן נָעוּל – אָדָם.\n",
        "הַאֵלֵךְ לִי? אוֹ אַכֶּה בַּסֶּלַע\n",
        "עַד זוֹב דָּם? \n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DIq9ATR89sa"
      },
      "source": [
        "# גרסה מאת דורון אדלר\n",
        "poem = shirbert('''\n",
        "רחל - גן נעול\n",
        "\n",
        "מי אתה? מדוע יד מושטת?\n",
        "לא פוגשת יד אחות\n",
        "ועיניים, אך תמתנה רגע\n",
        "והנה שפלו כבר נבוכות.\n",
        "\n",
        "גן נעול,\n",
        "לא שביל אליו, לא דרך.\n",
        "גם נעול, אדם, גן נעול.\n",
        "\n",
        "ההלך לי? \n",
        "או אכה בסלע?\n",
        "\n",
        "עד זוב דם\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umLERjig_UYN"
      },
      "source": [
        "poem=shirbert('''\n",
        "רק על עצמי\n",
        "\n",
        "רַק עַל עַצְמִי לְסַפֵּר יָדַעְתִּי.\n",
        "צַר עוֹלָמִי כְּעוֹלַם נְמָלָה,\n",
        "גַּם מַשָּׂאִי עָמַסְתִּי כָּמוֹהָ\n",
        "רַב וְכָבֵד מִכְּתֵפִי הַדַּלָּה.\n",
        "\n",
        "גַּם אֶת דַרְכִּי – כְּדַרְכָּהּ אֶל צַמֶּרֶת –\n",
        "דֶּרֶך מַכְאוֹב וְדֶרֶךְ עָמָל,\n",
        "יַד עֲנָקִים זְדוֹנָה וּבוֹטַחַת,\n",
        "יַד מִתְבַּדַּחַת שָׂמָה לְאַל.\n",
        "\n",
        "כָּל אָרְחוֹתַי הִלִּיז וְהִדְמִיע\n",
        "פַּחַד טָמִיר מִיַּד עֲנָקִים.\n",
        "לָמָּה קְרָאתֶם לִי, חוֹפֵי הַפֶּלֶא?\n",
        "לָמָּה כְּזַבְתֶּם, אוֹרוֹת רְחוֹקִים? \n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}