{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_5_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/workshop/blob/master/nn_5_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csvOItTNfHqk"
      },
      "source": [
        "# 5. Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mudsfWpldVGN"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.3\n",
        "!pip install livelossplot\n",
        "!pip install keras==2.2.5\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras import Input, Model\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import get_file\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from livelossplot import PlotLossesKeras\n",
        "import numpy as np\n",
        "import sys\n",
        "import re\n",
        "from collections import Counter\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fifPjuOxdVGe"
      },
      "source": [
        "# set random seeds for more reproducible results\n",
        "from numpy.random import seed\n",
        "seed(42)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(43)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHczAfijdVGm"
      },
      "source": [
        "path = get_file('trump.txt', origin='https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt')\n",
        "\n",
        "with open(path, encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    \n",
        "print(Counter(text))\n",
        "text = text.lower()\n",
        "text = re.sub('\\n+','\\n',text)\n",
        "text = re.sub(r'[^a-z0-9 \\n.,\\'\"?!$%-]','',text)\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvFaN3mqdVG2"
      },
      "source": [
        "chars = sorted(set(text))\n",
        "print('vocab size:', len(chars))\n",
        "char_indices = {c:i for i, c in enumerate(chars)}\n",
        "indices_char = {i:c for i, c in enumerate(chars)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIG9breGdVG_"
      },
      "source": [
        "# cut the text in semi-redundant sequences of max_seq_len characters\n",
        "max_seq_len = 40\n",
        "step = 3\n",
        "\n",
        "def get_seq(data):\n",
        "    sentences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(data) - max_seq_len, step):\n",
        "        sentences.append(data[i: i + max_seq_len])\n",
        "        next_chars.append(data[i + max_seq_len])\n",
        "    \n",
        "    # vectorize:\n",
        "    x = np.zeros((len(sentences), max_seq_len, len(chars)), dtype=np.bool)\n",
        "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[i, t, char_indices[char]] = 1\n",
        "        y[i, char_indices[next_chars[i]]] = 1\n",
        "    \n",
        "    return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU9ugd6zdVHI"
      },
      "source": [
        "split_idx = int(len(text)*0.9)\n",
        "train = text[:split_idx]\n",
        "valid = text[split_idx:]\n",
        "\n",
        "x,y = get_seq(train)\n",
        "print('number of sequences in train:', len(x))\n",
        "x_val,y_val = get_seq(valid)\n",
        "print('number of sequences in validation:', len(x_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0sbzN1OdVHR"
      },
      "source": [
        "# build the model\n",
        "def get_model(max_seq_len, vocab_size, nodes, depth=1):\n",
        "    inputs = Input(shape=(max_seq_len, vocab_size))\n",
        "    f = inputs\n",
        "    for i in range(depth):\n",
        "        f = LSTM(nodes, dropout=0.2, recurrent_dropout=0.2, return_sequences=i<depth-1)(f)\n",
        "    f = Dropout(0.2)(f)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(f)\n",
        "    return Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8obg9J_dVHX"
      },
      "source": [
        "#temperature = 0\n",
        "#temperature = 1\n",
        "temperature = 0.5\n",
        "\n",
        "def get_next_char(preds):\n",
        "     # these are necessary due to numerical precision issues:\n",
        "    preds = np.asarray(preds, dtype=np.float64)\n",
        "    preds = preds / np.sum(preds)\n",
        "    \n",
        "    # your code here:\n",
        "    if temperature == 0:\n",
        "        probas = preds\n",
        "    elif temperature == 1:\n",
        "        probas = np.random.multinomial(1, preds)\n",
        "    else:\n",
        "        # helper function to sample an index from a probability array\n",
        "        exp_preds = np.exp(np.log(preds+1e-6) / temperature)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds)\n",
        "    next_index = np.argmax(probas)\n",
        "    return indices_char[next_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vc_yzILdVHf"
      },
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    if epoch%10 != 0:\n",
        "        return\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    data = valid\n",
        "    \n",
        "    start_index = np.random.randint(0, len(data) - max_seq_len - 1)\n",
        "        \n",
        "    sentence = data[start_index: start_index + max_seq_len]\n",
        "    generated = sentence\n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    print()\n",
        "    sys.stdout.write(generated)\n",
        "\n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, max_seq_len, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_char = get_next_char(preds)\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmlEVFlLdVHp"
      },
      "source": [
        "optimizer = RMSprop(lr=0.01)\n",
        "batch_size = 128\n",
        "epochs = 21"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_njrwyGdVHy"
      },
      "source": [
        "model = get_model(max_seq_len, len(chars), 128, depth=1)\n",
        "print(model.summary())\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "seed(42)\n",
        "set_random_seed(43)\n",
        "\n",
        "history = model.fit(x, y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[print_callback], validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-VlbLcqdVH7"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ms-KO1DdVIE"
      },
      "source": [
        "model = get_model(max_seq_len, len(chars), 64, depth=2)\n",
        "print(model.summary())\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "seed(42)\n",
        "set_random_seed(43)\n",
        "\n",
        "history = model.fit(x, y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[print_callback], validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GzI5sifdVIK"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
