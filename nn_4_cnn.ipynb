{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_4_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/workshop/blob/master/nn_4_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPL9z9UsfEgd"
      },
      "source": [
        "# 4. Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaJB3RfDdoPl"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.3\n",
        "!pip install livelossplot\n",
        "!pip install keras==2.2.5\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from keras import Input, Model\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import mnist, cifar10\n",
        "from keras.utils import to_categorical\n",
        "from livelossplot import PlotLossesKeras\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq3kt-CcdoPu"
      },
      "source": [
        "# set random seeds for more reproducible results\n",
        "from numpy.random import seed\n",
        "seed(42)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(43)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFIQLJfAdoP4"
      },
      "source": [
        "# load dataset\n",
        "#(raw_x_train, raw_y_train), (raw_x_test, raw_y_test) = mnist.load_data()\n",
        "(raw_x_train, raw_y_train), (raw_x_test, raw_y_test) = cifar10.load_data()\n",
        "print(raw_x_train.shape, raw_y_train.shape)\n",
        "gray = True\n",
        "if len(raw_x_train.shape)==4 and gray:\n",
        "    raw_x_train = np.mean(raw_x_train, axis=-1, keepdims=True)\n",
        "    raw_x_test = np.mean(raw_x_test, axis=-1, keepdims=True)\n",
        "if len(raw_x_train.shape)==3:\n",
        "    raw_x_train = np.expand_dims(raw_x_train, axis=-1)\n",
        "    raw_x_test = np.expand_dims(raw_x_test, axis=-1)\n",
        "train_size = len(raw_y_train)\n",
        "test_size = len(raw_y_test)\n",
        "xdim = raw_x_train.shape[1]\n",
        "ydim = raw_x_train.shape[2]\n",
        "\n",
        "print(raw_x_train.dtype, raw_y_train.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOR0rcAvdoQD"
      },
      "source": [
        "print(raw_x_train[0])\n",
        "print(raw_y_train[0])\n",
        "if raw_x_train.shape[-1]==1:\n",
        "  plt.imshow(raw_x_train[0][...,0], cmap='gray')\n",
        "else:\n",
        "  plt.imshow(raw_x_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aByrOryzdoQL"
      },
      "source": [
        "print(np.min(raw_x_train), np.max(raw_x_train), np.median(raw_x_train))\n",
        "print(np.unique(raw_y_train, return_counts=True))\n",
        "print(np.unique(raw_y_test, return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OSVDe_sdoQR"
      },
      "source": [
        "n_classes = len(np.unique(raw_y_test))\n",
        "x_train, y_train = shuffle(raw_x_train, raw_y_train, random_state=44)\n",
        "x_train = x_train/255\n",
        "x_test = raw_x_test/255\n",
        "y_train = to_categorical(y_train, n_classes)\n",
        "y_test = to_categorical(raw_y_test, n_classes)\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVBUrCx1g9SM"
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "#helper function to plot in grids\n",
        "def show_grid(image_list,nrows,ncols,label_list=None,show_labels=False,savename=None,figsize=(10,10),showaxis='off'):\n",
        "    if type(image_list) is not list:\n",
        "        if(image_list.shape[-1]==1):\n",
        "            image_list = [image_list[i,:,:,0] for i in range(image_list.shape[0])]\n",
        "        elif(image_list.shape[-1]==3):\n",
        "            image_list = [image_list[i,:,:,:] for i in range(image_list.shape[0])]\n",
        "    fig = plt.figure(None, figsize,frameon=False)\n",
        "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                     nrows_ncols=(nrows, ncols),  # creates 2x2 grid of axes\n",
        "                     axes_pad=0.3,  # pad between axes in inch.\n",
        "                     share_all=True,\n",
        "                     )\n",
        "    for i in range(nrows*ncols):\n",
        "        ax = grid[i]\n",
        "        ax.imshow(image_list[i],cmap='Greys_r')  # The AxesGrid object work as a list of axes.\n",
        "        ax.axis('off')\n",
        "        if show_labels:\n",
        "            ax.set_title(label_list[i])\n",
        "    if savename != None:\n",
        "        plt.savefig(savename,bbox_inches='tight')\n",
        "\n",
        "show_grid(raw_x_train[:32],4,8,label_list=[class_names[i] for i in np.squeeze(raw_y_train[:32])],show_labels=True,figsize=(20,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgy7JjGgdoQd"
      },
      "source": [
        "def get_model():\n",
        "    # EX: your code here\n",
        "    \n",
        "\n",
        "    outputs = Dense(n_classes, activation='softmax')(f)\n",
        "    return Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "181nr_Sq9YUc"
      },
      "source": [
        "epochs = 20\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JaZWamydoQk"
      },
      "source": [
        "model = get_model()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[PlotLossesKeras()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpa8BPedoQ5"
      },
      "source": [
        "# Augmentations\n",
        "\n",
        "model = get_model()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "valid_idx = int(train_size*0.8)\n",
        "aug_train_data = datagen.flow(x=x_train[:valid_idx], y=y_train[:valid_idx], batch_size=batch_size, seed=45)\n",
        "valid_data = (x_train[valid_idx:], y_train[valid_idx:])\n",
        "history = model.fit_generator(aug_train_data, epochs=epochs, validation_data=valid_data, callbacks=[PlotLossesKeras()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzwkn83-doRA"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHNWmqPZdoRH"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "loss, acc = model.evaluate(x=x_test, y=y_test)\n",
        "print(loss, acc)\n",
        "targets = np.argmax(y_test, axis=-1)\n",
        "probabilities = model.predict(x=x_test)\n",
        "predictions = np.argmax(probabilities, axis=-1)\n",
        "cm = confusion_matrix(y_true=targets, y_pred=predictions)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpczivZJdoRN"
      },
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title = 'Normalized confusion matrix'\n",
        "    else:\n",
        "        title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    np.set_printoptions(precision=2)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "classes = np.arange(n_classes)\n",
        "plot_confusion_matrix(cm, classes=classes)\n",
        "plot_confusion_matrix(cm, classes=classes, normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BGouqTQdoRT"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_true=targets, y_pred=predictions, labels=classes, target_names=class_names)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYlQPvOWdoRd"
      },
      "source": [
        "# top errors\n",
        "from sklearn.metrics import log_loss\n",
        "max_probs = np.max(probabilities, axis=-1)\n",
        "losses = [log_loss(y_true=y, y_pred=prob, eps=1e-7) for y,prob in zip(y_test,probabilities)]\n",
        "print('loss\\tindex\\ttrue\\tpredicted\\tprobability')\n",
        "top_errors = sorted(list(zip(losses, np.arange(test_size), [class_names[i] for i in targets], [class_names[i] for i in predictions], max_probs)), reverse=True)[:32]\n",
        "titles = []\n",
        "inds = []\n",
        "for i,error in enumerate(top_errors):\n",
        "    titles.append('%s -> %s'%(error[2],error[3]))\n",
        "    inds.append(error[1])\n",
        "    if i<10:\n",
        "        print('%.04f\\t%d\\t%s\\t%s\\t\\t%.04f'%error)\n",
        "        #plt.figure()\n",
        "        #if raw_x_test.shape[-1]==1:\n",
        "        #    plt.imshow(raw_x_test[error[1]][..., 0], cmap='gray')\n",
        "        #else:\n",
        "        #    plt.imshow(raw_x_test[error[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qNJozLOhMpX"
      },
      "source": [
        "show_grid(raw_x_test[inds[:32]],4,8,label_list=titles[:32],show_labels=True,figsize=(20,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6lJpKFx5ZRF"
      },
      "source": [
        "# EX:\n",
        "# 1. do CIFAR-10 (gray) with a CNN:\n",
        "# a. start with the cifar solution to ex2\n",
        "# b. rewrite the model to use convoltions and max-pooling\n",
        "# c. don't optimize too much, just try to get a smaller validation error than ex2, with less degrees of freedom (see model.summary)\n",
        "# 2. do CIFAR-10 (rgb) with a CNN:\n",
        "# a. rewrite the code to deal with rgb channel\n",
        "# b. use an architecture of the form: conv-conv-maxpool-dropout-conv-conv-maxpool-dropout-flatten-dense-dropout-dense+softmax\n",
        "# c. look at the data, overfit, generalize (strive to validation accuracy>0.8), evaluate\n",
        "# d. calculate the number of degrees of freedom by hand (remember the biases), compare to model.summary(), show your calculation\n",
        "# e. add batch normalization after each conv-relu. does it help? was it worthwhile in terms of additional degrees of freedom and training time?\n",
        "# g. add horizontal flips, and small horizontal and vertical shifts random data augmentation. can you get better results?\n",
        "# h. bonus: there is an ongoing debate whether to put batch-norm before or after the relu. compare (you will need seperate the relu from the conv)"
      ]
    }
  ]
}
